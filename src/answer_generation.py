import subprocess

def get_answer(question, context):
    """
    Generate an answer using Llama2 based on the context.
    
    Args:
        question (str): The user's question.
        context (str): The relevant context from the document.
    
    Returns:
        str: The answer generated by Llama2.
    """
    if not context.strip():
        return "Sorry, this information is not mentioned in the given document."

    prompt = f"Provide a concise and factual answer to the question based on the context. If the information is not present, respond with 'Sorry, this information is not mentioned in the given document.'\n\nQuestion: {question}\nContext: {context}\nAnswer:"
    
    try:
        result = subprocess.run(
            ["ollama", "run", "llama2:latest", prompt],
            text=True,
            capture_output=True,
            encoding='utf-8'
        )
        
        if result.returncode != 0:
            raise RuntimeError(f"Ollama CLI error: {result.stderr}")
        
        answer = result.stdout.strip()
        
        if not answer or "Sorry" in answer:
            return "Sorry, this information is not mentioned in the given document."
        
        # Handle incomplete sentences or abrupt outputs
        if len(answer.split()) < 5:  # Arbitrary threshold for too-short answers
            return "Sorry, the generated answer appears incomplete or unclear."

        return answer  # Return full model output if it's coherent

    except Exception as e:
        return f"Error generating answer: {e}"
